{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional word representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of this taken from Christopher Potts' CS224u course from Spring 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__version__ = \"DS-GA 1012, NYU, Spring 2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this exercise and assignment, we want to explore creating vector representations of words (__distributed representations__) from co-occurence patterns in text.\n",
    "\n",
    "Note that the term __embedding__, __word vector__, or __word embedding__ can also be used for distributed representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a basic recipe for building a word $\\times$ word matrix:\n",
    "    \n",
    "0. Define a notion of co-occurrence context. This could be an entire document, a paragraph, a sentence, a clause, an NP â€” whatever domain seems likely to capture the associations you care about.\n",
    "0. Scan through your corpus building a dictionary $d$ mapping word-pairs to counts. Every time a pair of words $w$ and $w'$ occurs in the same context (as you defined it in 1),  increment $d[(w, w')]$ by $1$.\n",
    "0. Using the count dictionary $d$ that you collected in 2, establish your full vocabulary $V$, an ordered list of words types. For large collections of documents, $|V|$ will typically be huge. You will probably want to winnow the vocabulary at this point. You might do this by filtering to a specific subset, or just imposing a minimum count threshold. You might impose a minimum count threshold even if $|V|$ is small &mdash; for words with very low counts, you simply don't have enough evidence to say anything interesting.\n",
    "0. Now build a matrix $M$ of dimension $|V| \\times |V|$. Both the rows and the columns of $M$ represent words. Each cell $M[i, j]$ is filled with the count $d[(w_i, w_j)]$.\n",
    "\n",
    "For different designs, the procedure differs slightly. For example, if you are building a word $\\times$ document matrix, then the rows of $M$ represent words and the columns of $M$ represent documents. The scan in step 2 then just keeps track of (_word_, _document_) pairs &mdash; compiling the number of times that _word_ appears in _document_. Such matrices are often used in information retrieval, because the columns are multi-set representations of documents. They are much sparser than the the word $\\times$ word matrices we will work with here. (In my experience, they yield lower-quality lexicons, but others have reported good results with them.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice building a co-occurrence matrix. We'll use data from the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/). Download the main zip and extract the file `datasetSentences.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sst(data_file):\n",
    "    with open(data_file, 'r') as data_fh:\n",
    "        data_fh.readline() # skip the header\n",
    "        data = [r.split('\\t')[1] for r in data_fh.readlines()]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to build a co-occurrence matrix where words are considered co-oocurring if they are adjacent in a sentence. Let's implement this function.\n",
    "\n",
    "Some practical notes:\n",
    "- for the project, it's okay to use outside libraries, but it's good practice to implement things from scratch\n",
    "- try to stick to built-in Python data structures as much as possible, as using library data structures can add bloat and slow things down. Building the co-occurrence matrix should generally be pretty fast as it only requires one pass over the data, except instantiating the full matrix may be somewhat slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix():\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words...\n",
      "\tFinished counting words in 0.31881\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary in 0.00000\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 33.60083\n"
     ]
    }
   ],
   "source": [
    "data_file = 'stanfordSentimentTreebank/datasetSentences.txt'\n",
    "\n",
    "data = load_sst(data_file)\n",
    "mat, tok2idx, idx2tok = build_cooccurrence_matrix(data, max_vocab_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have distributed, distributional word representations! Each row of this co-occurrence matrix can be seen as a word vector, nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key operation on word vectors is being able to compare them. For the most part, we are interested in measuring the _distance_ between vectors. We surmise that semantically related words should be close together and semantically unrelated words should be far apart in the vector spaces we build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic and intuitive distance measure between vectors is __euclidean distance__. The euclidean distance between two vectors $u$ and $v$ of dimension $n$ is \n",
    "\n",
    "$$\\sqrt{\\sum_{i=1}^{n} |u_{i}-v_{i}|^2}$$ \n",
    "\n",
    "In two-dimensions, this corresponds to the length of the most direct line between the two points.\n",
    "\n",
    "As part of the exercise, implement this without using any other packages beyond what has been imported already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(u, v):    \n",
    "    \"\"\"Eculidean distance between 1d np.arrays `u` and `v`, which must \n",
    "    have the same dimensionality. Returns a float.\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a tiny vector space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ABC = np.array([\n",
    "    [ 1.0,  1.0],  # A\n",
    "    [9.0, 8.0],  # B\n",
    "    [10.0, 5.0]]) # C\n",
    "\n",
    "def plot_ABC(m):\n",
    "    plt.plot(m[:,0], m[:,1], marker='', linestyle='')\n",
    "    plt.xlim([0,np.max(m)*1.2])\n",
    "    plt.ylim([0,np.max(m)*1.2])\n",
    "    for i, x in enumerate(['A','B','C']):\n",
    "        plt.annotate(x, m[i,:])\n",
    "\n",
    "plot_ABC(ABC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The euclidean distances align well with the raw visual distance in the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.63014581273465"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean(ABC[0], ABC[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1622776601683795"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean(ABC[1], ABC[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we believed that vector B should be closer to vector A than to vector C. The Euclidean distances do not reflect that relationship.\n",
    "\n",
    "One strategy to impose this relationship is to normalize the vectors. We might normalize each vector by its __length__, which is defined for a vector $u$ of dimension $n$ as \n",
    "\n",
    "$$\\|u\\| = \\sqrt{\\sum_{i=1}^{n} u_{i}^{2}}$$ \n",
    "\n",
    "Please fill in `vector_length` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_length(u):\n",
    "    \"\"\"Length (L2) of the 1d np.array `u`. Returns a new np.array with the \n",
    "    same dimensions as `u`.\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our function to write a length-normalizing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_norm(u):\n",
    "    \"\"\"L2 norm of the 1d np.array `u`. Returns a float.\"\"\"\n",
    "    return u / vector_length(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization changes the affinity or distance between points A, B, and C dramatically. Is normalization the right thing to do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_ABC(np.array([length_norm(row) for row in ABC]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the connection between A and B is more apparent, as is the opposition between B and C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implicitly, we have replaced Euclidean distance as our distance metric with cosine distance. __Cosine distance__ takes overall length into account. The cosine distance between two vectors $u$ and $v$ of dimension $n$ is \n",
    "\n",
    "$$1 - \\left(\\frac{\\sum_{i=1}^{n} u_{i} \\cdot v_{i}}{\\|u\\|\\cdot \\|v\\|}\\right)$$\n",
    "\n",
    "The similarity part of this (the righthand term of the subtraction) is actually measuring the _angles_ between the two vectors. The result is the same (in terms of rank order) as one gets from first normalizing both vectors using `vector_length` and then calculating their Euclidean distance.\n",
    "\n",
    "Implement this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(u, v):        \n",
    "    \"\"\"Cosine distance between 1d np.arrays `u` and `v`, which must have \n",
    "    the same dimensionality. Returns a float.\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other distance measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very common to use Euclidean and cosine distance, but there are other distance measures one might consider using. For example, the particular definition of length we used is the L2-norm (because of the $^2$ exponent); we might consider using a different exponent such as $^1$, yielding the L1-norm.\n",
    "\n",
    "As practice, let's consider the Jaccard distance, which is defined as \n",
    "\n",
    "$$ J(u, v) = 1 - \\left( \\frac{\\sum_{i} min(u_i, v_i)}{\\sum_{i} max(u_i, v_i) } \\right) $$\n",
    "\n",
    "Jaccard distance measures how dissimilar two vectors are by comparing their \"intersection\" and \"union\" (it's usually defined in context of sets). Let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(u, v):\n",
    "    \"\"\"Jaccard similarity between two real-valued vectors\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recent line of work is in learning _hyperbolic_ word embeddings, where the embeddings live in a non-Euclidean, hyperbolic space. The appeal of this approach is that the learned embeddings can respect the hierarchical relationship that some words have with each other.\n",
    "\n",
    "For more information, see [Nickel and Kiela (2017)](https://arxiv.org/abs/1705.08039) or [Chamberlain et al. (2017)](https://arxiv.org/abs/1705.10359)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we set for ourselves the goal of associating A with B and disassociating B from C, in keeping with the semantic intuition expressed above. Then we can assess distance measures by whether they achieve this goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      euclidean(A, B) = 10.630       euclidean(B, C) = 3.162\n",
      "         cosine(A, B) = 0.002          cosine(B, C) = 0.034\n",
      "        jaccard(A, B) = 0.882         jaccard(B, C) = 0.222\n"
     ]
    }
   ],
   "source": [
    "for m in (euclidean, cosine, jaccard):\n",
    "    fmt = {'n': m.__name__,  \n",
    "           'AB': m(ABC[0], ABC[1]), \n",
    "           'BC': m(ABC[1], ABC[2])}\n",
    "    print('%(n)15s(A, B) = %(AB)5.3f %(n)15s(B, C) = %(BC)5.3f' % fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `neighbors` function is an investigative aide. For a given `word`, it ranks all the words in the vocabulary `rownames` according to their distance from `word`, as measured by `distfunc` in matrix `mat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbors(tok, mat, tok2idx, idx2tok, distfunc=cosine):    \n",
    "    \"\"\"Tool for finding the nearest neighbors of `word` in `mat` according \n",
    "    to `distfunc`. The comparisons are between row vectors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tok : str\n",
    "        The anchor word. Assumed to be in `tok2idx`.\n",
    "        \n",
    "    mat : np.array\n",
    "        The vector-space model.\n",
    "        \n",
    "    tok2idx : list of str\n",
    "        The rownames of mat.\n",
    "            \n",
    "    distfunc : function mapping vector pairs to floats (default: `cosine`)\n",
    "        The measure of distance between vectors. Can also be `euclidean`, \n",
    "        `matching`, `jaccard`, as well as any other distance measure  \n",
    "        between 1d vectors.\n",
    "    \n",
    "    Returns\n",
    "    -------    \n",
    "    list of tuples\n",
    "        The list is ordered by closeness to `word`. Each member is a pair \n",
    "        (word, distance) where word is a str and distance is a float.\n",
    "    \n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By playing around with this function, you can start to get a sense for how the distance functions differ. Here are some example calls; you might try some new words to get a feel for what these matrices are like and how different words look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('superb', 0.0),\n",
       " ('cipher', 4.69041575982343),\n",
       " ('illuminated', 4.69041575982343),\n",
       " ('Using', 4.795831523312719),\n",
       " ('spine', 4.795831523312719),\n",
       " ('legend', 4.898979485566356),\n",
       " ('shining', 4.898979485566356),\n",
       " ('lick', 4.898979485566356),\n",
       " ('bravura', 4.898979485566356),\n",
       " ('stellar', 4.898979485566356)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(tok='superb', mat=mat, tok2idx=tok2idx, idx2tok=idx2tok, distfunc=euclidean)[: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/miniconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('superb', 1.1102230246251565e-16),\n",
       " ('cipher', 0.5219085562662426),\n",
       " ('fellow', 0.5271337562565397),\n",
       " ('illuminated', 0.527544408738466),\n",
       " ('terrific', 0.5281792278953805),\n",
       " ('solid', 0.538018447402505),\n",
       " ('wonderful', 0.5496408554571595),\n",
       " ('fault', 0.5552504100033393),\n",
       " ('good', 0.560877926872458),\n",
       " ('Using', 0.5635642195280153)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(tok='superb', mat=mat, tok2idx=tok2idx, idx2tok=idx2tok, distfunc=cosine)[: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rankings are okay, with `cosine` less likely to associate words that happen to have similar frequency, but somewhat noisy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix reweighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweighting aims to amplify the important, trustworthy, and unusual, while deemphasizing the mundane and the quirky. The intuition behind moving away from raw counts is that just using frequency is too fuzzy a concept for our goal of encoding semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization (row-wise or column-wise) is perhaps the simplest form of reweighting. With [length_norm](#Length-normalization), we normalize using `vector_length`. We can also normalize each row by the sum of its values, which turns each row into a probability distribution over the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_norm(u):\n",
    "    \"\"\"Normalize 1d np.array `u` into a probability distribution. Assumes \n",
    "    that all the members of `u` are positive. Returns a 1d np.array of \n",
    "    the same dimensionality as `u`.\"\"\"\n",
    "    return u / np.sum(u)\n",
    "\n",
    "def rowwise_norm_mat(mat):\n",
    "    return np.array([prob_norm(u) for u in mat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/miniconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "norm_mat = rowwise_norm_mat(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key point: These normalization measures are insensitive to the _magnitude_ of the underlying counts. \n",
    "\n",
    "This is often a mistake in the messy world of large data sets; $[1,10]$ and $[1000,10000]$ are very different in ways that will be partly or totally obscured by normalization.\n",
    "\n",
    "How do we solve this? Pointwise mutual information (see homework)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above methods deliver solid results. However, they are not capable of capturing higher-order associations in the data. For example, both _gnarly_ and _wicked_ are used as slangily positive adjectives. We thus expect them to have many of the same neighbors. However, at least stereotypically, _gnarly_ is Californian and _wicked_ is Bostonian. Thus, they are unlikely \n",
    "to occur often in the same texts. Dimensionality reduction techniques are often capable of capturing their semantic similarity (and have the added advantage of shrinking the size of our data structures).\n",
    "\n",
    "The general goal of dimensionality reduction is to eliminate rows/columns that are highly correlated while bringing similar things together and pushing dissimilar things apart. __Latent Semantic Analysis__ (LSA) is a prominent method. It is an application of truncated __singular value decomposition__ (SVD). SVD is a central matrix operation; 'truncation' here means looking only at submatrices of the full decomposition. LSA seeks not only to find a reduced-sized matrix but also to capture similarities that come not just from direct co-occurrence, but also from second-order co-occurrence.\n",
    "\n",
    "Dimensionality reduction is also particularly helpful in our case because our vectors are very sparse. Intuitively, many words don't often appear right next to each other, so many entries in the word vectors are zero. By reducing our word vectors, we won't need to pass around very large vectors and remove some of the sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa(mat=None, k=100):\n",
    "    \"\"\"Latent Semantic Analysis using pure scipy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mat : 2d np.array\n",
    "       The matrix to operate on.\n",
    "        \n",
    "    k : int (default: 100)\n",
    "        Number of dimensions to truncate to.\n",
    "        \n",
    "    Returns\n",
    "    -------    \n",
    "    (np.array, list of str)\n",
    "        The first member is the SVD-reduced version of `mat` with \n",
    "        dimension (m x k), where m is the rowcount of mat and `k` is \n",
    "        either the user-supplied k or the column count of `mat`, whichever \n",
    "        is smaller. The second member is `rownames` (unchanged).\n",
    "\n",
    "    \"\"\"    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a look at the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnmat = np.array([\n",
    "    [1,0,1,0,0,0],\n",
    "    [0,1,0,1,0,0],\n",
    "    [1,1,1,1,0,0],\n",
    "    [0,0,0,0,1,1],\n",
    "    [0,0,0,0,0,1]], dtype='float64')\n",
    "gn_idx2tok = {0:'gnarly', 1:'wicked', 2:'awesome', 3:'lame', 4:'terrible'}\n",
    "gn_tok2idx = {v: k for k, v in gn_idx2tok.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gnarly', 2.220446049250313e-16),\n",
       " ('awesome', 0.29289321881345254),\n",
       " ('wicked', 1.0),\n",
       " ('lame', 1.0),\n",
       " ('terrible', 1.0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(tok='gnarly', mat=gnmat, tok2idx=gn_tok2idx, idx2tok=gn_idx2tok, distfunc=cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that _gnarly_ and _wicked_ are not close to each other. (Well, it's a small space, but they are as close as _gnarly_ and _lame_.) Reweighting by PMI, PPMI, or TF-IDF is no help. LSA to the rescue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnmat_lsa = lsa(mat=gnmat, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gnarly', 0.0),\n",
       " ('wicked', 0.0),\n",
       " ('awesome', 0.0),\n",
       " ('lame', 1.0),\n",
       " ('terrible', 1.0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors(tok='gnarly', mat=gnmat_lsa, tok2idx=gn_tok2idx, idx2tok=gn_idx2tok, distfunc=cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've done a good deal of work creating these word vectors, but how do we know how good they are? Inspecting the neighbors of words according to their vectors and making sure they match our intuition is a good hueristic.\n",
    "\n",
    "Expanding on this idea, we can evaluate the quality of word vectors by using them to compute similarities of pairs of words and then comparing those similarities to human judgments. Let's use this strategy to evaluate the word vectors we just learned. We'll use the [MTurk-771](http://www2.mta.ac.il/~gideon/mturk771.html) word similarity dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_similarity_dataset(data_file):\n",
    "    with open(data_file, 'r') as data_fh:\n",
    "        raw_data = data_fh.readlines()\n",
    "    data = []\n",
    "    trgs = []\n",
    "    for datum in raw_data:\n",
    "        datum = datum.strip().split(',')\n",
    "        data.append((datum[0], datum[1]))\n",
    "        trgs.append(float(datum[2]))\n",
    "    return data, trgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def evaluate_word_similarity():\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'MTURK-771.csv'\n",
    "test_data, test_trgs = load_word_similarity_dataset(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 248 of 771 examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02485435870523964"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_word_similarity(test_data, test_trgs, norm_mat, tok2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our word vectors aren't very good (NLP isn't that easy...). There are some great pre-computed matrices available online too. These aren't matrices of counts, but rather more abstract values computed using methods like those under discussion here. [GloVe](https://nlp.stanford.edu/projects/glove/) is a unsupervised learning algorithm that learns vector representations of words using word-to-word cooccurence matrices from a corpus. \n",
    "\n",
    "Write code to load the GloVe vectors (you don't necessarily need to load *all* of them) and evaluate them on MTurk-771. To receive the ~*bonus*~ (0.2 points on your overall grade), show your code to the instructor and the value when evaluating the first 50,000 GloVe word vectors on MTurk-771 (ignoring OOV examples)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
